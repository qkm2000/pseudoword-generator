{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an decoder model which will take in a roundness value and output a pseudoword that corresponds to the roundness value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pseudoword_generator import *\n",
    "from utils.word_tokenizer import *\n",
    "from dotenv import load_dotenv\n",
    "from utils.dataset import *\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "pd.set_option('display.max_columns', None)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is obtained from normalizing the dataset from `datasets\\Fortetal2015_dataforOSF.csv` and the processing code can be found in `data_normalizer.ipynb`\n",
    "\n",
    "A linear normalization is performed to transform the `ExperimentalRoundScore` from the original (-42 to 50) to (0 to 1), with 0 being maximally round and 1 being maximally sharp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pseudoword</th>\n",
       "      <th>Roundness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bebi</td>\n",
       "      <td>0.815217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bibe</td>\n",
       "      <td>0.913043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bobou</td>\n",
       "      <td>0.815217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boubo</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chechi</td>\n",
       "      <td>0.184783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>outou</td>\n",
       "      <td>0.347826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>uku</td>\n",
       "      <td>0.239130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>ulu</td>\n",
       "      <td>0.913043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>umu</td>\n",
       "      <td>0.913043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>utu</td>\n",
       "      <td>0.239130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pseudoword  Roundness\n",
       "0         bebi   0.815217\n",
       "1         bibe   0.913043\n",
       "2        bobou   0.815217\n",
       "3        boubo   1.000000\n",
       "4       chechi   0.184783\n",
       "..         ...        ...\n",
       "119      outou   0.347826\n",
       "120        uku   0.239130\n",
       "121        ulu   0.913043\n",
       "122        umu   0.913043\n",
       "123        utu   0.239130\n",
       "\n",
       "[124 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataset\n",
    "data = pd.read_csv(f\"datasets/normalized.csv\")\n",
    "data.rename(columns={\"Stimuli\": \"Pseudoword\", \"ExperimentalRoundScore\": \"Roundness\"}, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Roundness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>124.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.562675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.316366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.260870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.543478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.902174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Roundness\n",
       "count  124.000000\n",
       "mean     0.562675\n",
       "std      0.316366\n",
       "min      0.000000\n",
       "25%      0.260870\n",
       "50%      0.543478\n",
       "75%      0.902174\n",
       "max      1.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn, val, tst = create_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparam tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section performs a grid search to determine what are the best parameters to use to train the model. These intermediary models are trained with a smaller number of epochs and a shorter early stopping patience, as we are only looking to see which hyperparameters are the best\n",
    "\n",
    "The best model is defined here as the model with the lowest test score. Since the test set is never seen by the model in training, it can be said that if the model performs well on the test set, it is more generalizable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'd_model': [64, 128],\n",
    "    'nhead': [8, 16],\n",
    "    'num_layers': [8, 16],\n",
    "    'learning_rate': [0.1, 0.05],\n",
    "    'weight_decay': [0.01],\n",
    "    'batch_size': [8],\n",
    "    'max_length': [16]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/16] Testing parameters: {'d_model': 64, 'nhead': 8, 'num_layers': 8, 'learning_rate': 0.1, 'weight_decay': 0.01, 'batch_size': 8, 'max_length': 16}\n",
      "Using decoupled weight decay\n",
      "Epoch 1: Average Training Loss: 3.4642, Average Validation Loss: 3.4732\n",
      "Epoch 2: Average Training Loss: 3.2958, Average Validation Loss: 2.9094\n",
      "Epoch 3: Average Training Loss: 2.8217, Average Validation Loss: 2.4684\n",
      "Epoch 4: Average Training Loss: 2.4561, Average Validation Loss: 2.2023\n",
      "Epoch 5: Average Training Loss: 2.2808, Average Validation Loss: 2.1097\n",
      "Epoch 6: Average Training Loss: 2.2001, Average Validation Loss: 2.0974\n",
      "Epoch 7: Average Training Loss: 2.1021, Average Validation Loss: 2.0468\n",
      "Epoch 8: Average Training Loss: 2.0711, Average Validation Loss: 1.9833\n",
      "Epoch 9: Average Training Loss: 2.0413, Average Validation Loss: 2.0219\n",
      "Epoch 10: Average Training Loss: 1.9987, Average Validation Loss: 2.0081\n",
      "Epoch 11: Average Training Loss: 1.9866, Average Validation Loss: 2.0060\n",
      "\n",
      "Early stopping triggered after epoch 11\n",
      "Best Validation Loss: 1.9833\n",
      "\n",
      "Evaluating on test set...\n",
      "Final Test Results:\n",
      "Average Test Loss: 2.0664\n",
      "\n",
      "[2/16] Testing parameters: {'d_model': 64, 'nhead': 8, 'num_layers': 8, 'learning_rate': 0.05, 'weight_decay': 0.01, 'batch_size': 8, 'max_length': 16}\n",
      "Using decoupled weight decay\n",
      "Epoch 1: Average Training Loss: 3.5262, Average Validation Loss: 3.5805\n",
      "Epoch 2: Average Training Loss: 3.5091, Average Validation Loss: 3.5042\n",
      "Epoch 3: Average Training Loss: 3.4142, Average Validation Loss: 3.2711\n",
      "Epoch 4: Average Training Loss: 3.1067, Average Validation Loss: 2.9424\n",
      "Epoch 5: Average Training Loss: 2.7625, Average Validation Loss: 2.5247\n",
      "Epoch 6: Average Training Loss: 2.4663, Average Validation Loss: 2.3275\n",
      "Epoch 7: Average Training Loss: 2.3032, Average Validation Loss: 2.1416\n",
      "Epoch 8: Average Training Loss: 2.2066, Average Validation Loss: 2.1258\n",
      "Epoch 9: Average Training Loss: 2.1544, Average Validation Loss: 2.1075\n",
      "Epoch 10: Average Training Loss: 2.0977, Average Validation Loss: 2.0416\n",
      "Epoch 11: Average Training Loss: 2.0656, Average Validation Loss: 2.0510\n",
      "Epoch 12: Average Training Loss: 2.0578, Average Validation Loss: 2.0089\n",
      "Epoch 13: Average Training Loss: 2.0244, Average Validation Loss: 2.0056\n",
      "Epoch 14: Average Training Loss: 2.0020, Average Validation Loss: 1.9567\n",
      "Epoch 15: Average Training Loss: 1.9786, Average Validation Loss: 1.9905\n",
      "\n",
      "Evaluating on test set...\n",
      "Final Test Results:\n",
      "Average Test Loss: 2.0705\n",
      "\n",
      "[3/16] Testing parameters: {'d_model': 64, 'nhead': 8, 'num_layers': 16, 'learning_rate': 0.1, 'weight_decay': 0.01, 'batch_size': 8, 'max_length': 16}\n",
      "Using decoupled weight decay\n",
      "Epoch 1: Average Training Loss: 3.5412, Average Validation Loss: 3.5768\n",
      "Epoch 2: Average Training Loss: 3.3348, Average Validation Loss: 3.0607\n",
      "Epoch 3: Average Training Loss: 2.8986, Average Validation Loss: 2.5445\n",
      "Epoch 4: Average Training Loss: 2.5360, Average Validation Loss: 2.2695\n",
      "Epoch 5: Average Training Loss: 2.3648, Average Validation Loss: 2.1391\n",
      "Epoch 6: Average Training Loss: 2.2359, Average Validation Loss: 2.1864\n",
      "Epoch 7: Average Training Loss: 2.1987, Average Validation Loss: 2.0990\n",
      "Epoch 8: Average Training Loss: 2.1626, Average Validation Loss: 2.1474\n",
      "Epoch 9: Average Training Loss: 2.1463, Average Validation Loss: 2.1437\n",
      "Epoch 10: Average Training Loss: 2.0966, Average Validation Loss: 2.0589\n",
      "Epoch 11: Average Training Loss: 2.1381, Average Validation Loss: 2.0349\n",
      "Epoch 12: Average Training Loss: 2.0918, Average Validation Loss: 2.0839\n",
      "Epoch 13: Average Training Loss: 2.0608, Average Validation Loss: 2.1142\n",
      "Epoch 14: Average Training Loss: 2.0574, Average Validation Loss: 2.0199\n",
      "Epoch 15: Average Training Loss: 2.0281, Average Validation Loss: 1.9445\n",
      "\n",
      "Evaluating on test set...\n",
      "Final Test Results:\n",
      "Average Test Loss: 2.1488\n",
      "\n",
      "[4/16] Testing parameters: {'d_model': 64, 'nhead': 8, 'num_layers': 16, 'learning_rate': 0.05, 'weight_decay': 0.01, 'batch_size': 8, 'max_length': 16}\n",
      "Using decoupled weight decay\n",
      "Epoch 1: Average Training Loss: 3.7161, Average Validation Loss: 3.7401\n",
      "Epoch 2: Average Training Loss: 3.6760, Average Validation Loss: 3.6835\n",
      "Epoch 3: Average Training Loss: 3.4988, Average Validation Loss: 3.2998\n",
      "Epoch 4: Average Training Loss: 3.0928, Average Validation Loss: 2.8966\n",
      "Epoch 5: Average Training Loss: 2.8624, Average Validation Loss: 2.6001\n",
      "Epoch 6: Average Training Loss: 2.6116, Average Validation Loss: 2.4253\n",
      "Epoch 7: Average Training Loss: 2.4080, Average Validation Loss: 2.2722\n",
      "Epoch 8: Average Training Loss: 2.3008, Average Validation Loss: 2.2280\n",
      "Epoch 9: Average Training Loss: 2.2178, Average Validation Loss: 2.1389\n",
      "Epoch 10: Average Training Loss: 2.1949, Average Validation Loss: 2.1534\n",
      "Epoch 11: Average Training Loss: 2.1742, Average Validation Loss: 2.0235\n",
      "Epoch 12: Average Training Loss: 2.1000, Average Validation Loss: 2.0199\n",
      "Epoch 13: Average Training Loss: 2.0976, Average Validation Loss: 2.0533\n",
      "Epoch 14: Average Training Loss: 2.1074, Average Validation Loss: 2.0216\n",
      "Epoch 15: Average Training Loss: 2.0971, Average Validation Loss: 2.0772\n",
      "\n",
      "Early stopping triggered after epoch 15\n",
      "Best Validation Loss: 2.0199\n",
      "\n",
      "Evaluating on test set...\n",
      "Final Test Results:\n",
      "Average Test Loss: 2.0782\n",
      "\n",
      "[5/16] Testing parameters: {'d_model': 64, 'nhead': 16, 'num_layers': 8, 'learning_rate': 0.1, 'weight_decay': 0.01, 'batch_size': 8, 'max_length': 16}\n",
      "Using decoupled weight decay\n",
      "Epoch 1: Average Training Loss: 3.4657, Average Validation Loss: 3.3644\n",
      "Epoch 2: Average Training Loss: 3.3134, Average Validation Loss: 2.9268\n",
      "Epoch 3: Average Training Loss: 2.7975, Average Validation Loss: 2.4620\n",
      "Epoch 4: Average Training Loss: 2.3624, Average Validation Loss: 2.1345\n",
      "Epoch 5: Average Training Loss: 2.2103, Average Validation Loss: 2.0854\n",
      "Epoch 6: Average Training Loss: 2.1450, Average Validation Loss: 2.0658\n",
      "Epoch 7: Average Training Loss: 2.0878, Average Validation Loss: 2.0195\n",
      "Epoch 8: Average Training Loss: 2.0756, Average Validation Loss: 1.9597\n",
      "Epoch 9: Average Training Loss: 2.0441, Average Validation Loss: 1.9780\n",
      "Epoch 10: Average Training Loss: 2.0405, Average Validation Loss: 1.9417\n",
      "Epoch 11: Average Training Loss: 1.9879, Average Validation Loss: 1.8815\n",
      "Epoch 12: Average Training Loss: 1.9521, Average Validation Loss: 2.0258\n",
      "Epoch 13: Average Training Loss: 1.9502, Average Validation Loss: 1.8833\n",
      "Epoch 14: Average Training Loss: 1.9347, Average Validation Loss: 1.8809\n",
      "Epoch 15: Average Training Loss: 1.9212, Average Validation Loss: 1.8162\n",
      "\n",
      "Evaluating on test set...\n",
      "Final Test Results:\n",
      "Average Test Loss: 2.0789\n",
      "\n",
      "[6/16] Testing parameters: {'d_model': 64, 'nhead': 16, 'num_layers': 8, 'learning_rate': 0.05, 'weight_decay': 0.01, 'batch_size': 8, 'max_length': 16}\n",
      "Using decoupled weight decay\n",
      "Epoch 1: Average Training Loss: 3.3840, Average Validation Loss: 3.4118\n",
      "Epoch 2: Average Training Loss: 3.3824, Average Validation Loss: 3.4178\n",
      "Epoch 3: Average Training Loss: 3.2586, Average Validation Loss: 3.0498\n",
      "Epoch 4: Average Training Loss: 2.8452, Average Validation Loss: 2.5402\n",
      "Epoch 5: Average Training Loss: 2.5227, Average Validation Loss: 2.2651\n",
      "Epoch 6: Average Training Loss: 2.3260, Average Validation Loss: 2.2153\n",
      "Epoch 7: Average Training Loss: 2.2210, Average Validation Loss: 2.1225\n",
      "Epoch 8: Average Training Loss: 2.1413, Average Validation Loss: 2.0754\n",
      "Epoch 9: Average Training Loss: 2.1249, Average Validation Loss: 2.0347\n",
      "Epoch 10: Average Training Loss: 2.0744, Average Validation Loss: 1.9897\n",
      "Epoch 11: Average Training Loss: 2.0469, Average Validation Loss: 2.0740\n",
      "Epoch 12: Average Training Loss: 2.0316, Average Validation Loss: 1.9194\n",
      "Epoch 13: Average Training Loss: 2.0101, Average Validation Loss: 2.0119\n",
      "Epoch 14: Average Training Loss: 1.9921, Average Validation Loss: 1.9501\n",
      "Epoch 15: Average Training Loss: 1.9885, Average Validation Loss: 1.9464\n",
      "\n",
      "Early stopping triggered after epoch 15\n",
      "Best Validation Loss: 1.9194\n",
      "\n",
      "Evaluating on test set...\n",
      "Final Test Results:\n",
      "Average Test Loss: 2.0671\n",
      "\n",
      "[7/16] Testing parameters: {'d_model': 64, 'nhead': 16, 'num_layers': 16, 'learning_rate': 0.1, 'weight_decay': 0.01, 'batch_size': 8, 'max_length': 16}\n",
      "Using decoupled weight decay\n",
      "Epoch 1: Average Training Loss: 3.6079, Average Validation Loss: 3.4038\n",
      "Epoch 2: Average Training Loss: 3.3512, Average Validation Loss: 3.0555\n",
      "Epoch 3: Average Training Loss: 2.9543, Average Validation Loss: 2.6847\n",
      "Epoch 4: Average Training Loss: 2.5545, Average Validation Loss: 2.3326\n",
      "Epoch 5: Average Training Loss: 2.3220, Average Validation Loss: 2.2056\n",
      "Epoch 6: Average Training Loss: 2.2450, Average Validation Loss: 2.1012\n",
      "Epoch 7: Average Training Loss: 2.1753, Average Validation Loss: 2.0787\n",
      "Epoch 8: Average Training Loss: 2.1457, Average Validation Loss: 2.0695\n",
      "Epoch 9: Average Training Loss: 2.0788, Average Validation Loss: 2.0841\n",
      "Epoch 10: Average Training Loss: 2.0896, Average Validation Loss: 2.0867\n",
      "Epoch 11: Average Training Loss: 2.0553, Average Validation Loss: 2.0046\n",
      "Epoch 12: Average Training Loss: 2.0536, Average Validation Loss: 2.0294\n",
      "Epoch 13: Average Training Loss: 2.0342, Average Validation Loss: 1.9991\n",
      "Epoch 14: Average Training Loss: 2.0168, Average Validation Loss: 1.9413\n",
      "Epoch 15: Average Training Loss: 2.0064, Average Validation Loss: 2.0009\n",
      "\n",
      "Evaluating on test set...\n",
      "Final Test Results:\n",
      "Average Test Loss: 2.1314\n",
      "\n",
      "[8/16] Testing parameters: {'d_model': 64, 'nhead': 16, 'num_layers': 16, 'learning_rate': 0.05, 'weight_decay': 0.01, 'batch_size': 8, 'max_length': 16}\n",
      "Using decoupled weight decay\n",
      "Epoch 1: Average Training Loss: 3.4523, Average Validation Loss: 3.3862\n",
      "Epoch 2: Average Training Loss: 3.4377, Average Validation Loss: 3.3256\n",
      "Epoch 3: Average Training Loss: 3.3578, Average Validation Loss: 3.1740\n",
      "Epoch 4: Average Training Loss: 3.0798, Average Validation Loss: 2.8155\n",
      "Epoch 5: Average Training Loss: 2.7100, Average Validation Loss: 2.3933\n",
      "Epoch 6: Average Training Loss: 2.4036, Average Validation Loss: 2.2955\n",
      "Epoch 7: Average Training Loss: 2.2703, Average Validation Loss: 2.1523\n",
      "Epoch 8: Average Training Loss: 2.1836, Average Validation Loss: 2.1060\n",
      "Epoch 9: Average Training Loss: 2.1672, Average Validation Loss: 2.0669\n",
      "Epoch 10: Average Training Loss: 2.1241, Average Validation Loss: 2.0161\n",
      "Epoch 11: Average Training Loss: 2.1147, Average Validation Loss: 2.0693\n",
      "Epoch 12: Average Training Loss: 2.0767, Average Validation Loss: 2.0282\n",
      "Epoch 13: Average Training Loss: 2.0831, Average Validation Loss: 1.9764\n",
      "Epoch 14: Average Training Loss: 2.0465, Average Validation Loss: 1.9566\n",
      "Epoch 15: Average Training Loss: 2.0355, Average Validation Loss: 1.9684\n",
      "\n",
      "Evaluating on test set...\n",
      "Final Test Results:\n",
      "Average Test Loss: 2.1185\n",
      "\n",
      "[9/16] Testing parameters: {'d_model': 128, 'nhead': 8, 'num_layers': 8, 'learning_rate': 0.1, 'weight_decay': 0.01, 'batch_size': 8, 'max_length': 16}\n",
      "Using decoupled weight decay\n",
      "Epoch 1: Average Training Loss: 3.3744, Average Validation Loss: 3.2615\n",
      "Epoch 2: Average Training Loss: 3.1841, Average Validation Loss: 2.6475\n",
      "Epoch 3: Average Training Loss: 2.4920, Average Validation Loss: 2.1058\n",
      "Epoch 4: Average Training Loss: 2.1579, Average Validation Loss: 1.9983\n",
      "Epoch 5: Average Training Loss: 2.0773, Average Validation Loss: 2.0302\n",
      "Epoch 6: Average Training Loss: 2.0241, Average Validation Loss: 1.9190\n",
      "Epoch 7: Average Training Loss: 1.9825, Average Validation Loss: 1.9837\n",
      "Epoch 8: Average Training Loss: 1.9797, Average Validation Loss: 1.8636\n",
      "Epoch 9: Average Training Loss: 1.9610, Average Validation Loss: 1.8688\n",
      "Epoch 10: Average Training Loss: 1.9209, Average Validation Loss: 1.8997\n",
      "Epoch 11: Average Training Loss: 1.9008, Average Validation Loss: 1.8531\n",
      "Epoch 12: Average Training Loss: 1.9065, Average Validation Loss: 1.7772\n",
      "Epoch 13: Average Training Loss: 1.8902, Average Validation Loss: 1.8361\n",
      "Epoch 14: Average Training Loss: 1.9123, Average Validation Loss: 1.9063\n",
      "Epoch 15: Average Training Loss: 1.8620, Average Validation Loss: 1.8850\n",
      "\n",
      "Early stopping triggered after epoch 15\n",
      "Best Validation Loss: 1.7772\n",
      "\n",
      "Evaluating on test set...\n",
      "Final Test Results:\n",
      "Average Test Loss: 2.0489\n",
      "\n",
      "[10/16] Testing parameters: {'d_model': 128, 'nhead': 8, 'num_layers': 8, 'learning_rate': 0.05, 'weight_decay': 0.01, 'batch_size': 8, 'max_length': 16}\n",
      "Using decoupled weight decay\n",
      "Epoch 1: Average Training Loss: 3.6409, Average Validation Loss: 3.6923\n",
      "Epoch 2: Average Training Loss: 3.6209, Average Validation Loss: 3.6566\n",
      "Epoch 3: Average Training Loss: 3.4580, Average Validation Loss: 3.3273\n",
      "Epoch 4: Average Training Loss: 2.9302, Average Validation Loss: 2.4879\n",
      "Epoch 5: Average Training Loss: 2.3840, Average Validation Loss: 2.0766\n",
      "Epoch 6: Average Training Loss: 2.1661, Average Validation Loss: 2.1174\n",
      "Epoch 7: Average Training Loss: 2.1278, Average Validation Loss: 2.0258\n",
      "Epoch 8: Average Training Loss: 2.0596, Average Validation Loss: 1.9716\n",
      "Epoch 9: Average Training Loss: 2.0126, Average Validation Loss: 1.8666\n",
      "Epoch 10: Average Training Loss: 1.9412, Average Validation Loss: 1.8524\n",
      "Epoch 11: Average Training Loss: 1.9565, Average Validation Loss: 1.8709\n",
      "Epoch 12: Average Training Loss: 1.9302, Average Validation Loss: 1.9045\n",
      "Epoch 13: Average Training Loss: 1.9066, Average Validation Loss: 1.8518\n",
      "Epoch 14: Average Training Loss: 1.9122, Average Validation Loss: 1.8399\n",
      "Epoch 15: Average Training Loss: 1.9045, Average Validation Loss: 1.9230\n",
      "\n",
      "Evaluating on test set...\n",
      "Final Test Results:\n",
      "Average Test Loss: 2.0354\n",
      "\n",
      "[11/16] Testing parameters: {'d_model': 128, 'nhead': 8, 'num_layers': 16, 'learning_rate': 0.1, 'weight_decay': 0.01, 'batch_size': 8, 'max_length': 16}\n",
      "Using decoupled weight decay\n",
      "Epoch 1: Average Training Loss: 3.3714, Average Validation Loss: 3.2174\n",
      "Epoch 2: Average Training Loss: 3.1802, Average Validation Loss: 2.6896\n",
      "Epoch 3: Average Training Loss: 2.6162, Average Validation Loss: 2.3952\n",
      "Epoch 4: Average Training Loss: 2.3565, Average Validation Loss: 2.1776\n",
      "Epoch 5: Average Training Loss: 2.2078, Average Validation Loss: 2.0746\n",
      "Epoch 6: Average Training Loss: 2.1340, Average Validation Loss: 2.0922\n",
      "Epoch 7: Average Training Loss: 2.0772, Average Validation Loss: 2.0126\n",
      "Epoch 8: Average Training Loss: 2.0333, Average Validation Loss: 1.9451\n",
      "Epoch 9: Average Training Loss: 1.9909, Average Validation Loss: 1.9082\n",
      "Epoch 10: Average Training Loss: 1.9249, Average Validation Loss: 1.9313\n",
      "Epoch 11: Average Training Loss: 1.9540, Average Validation Loss: 1.8330\n",
      "Epoch 12: Average Training Loss: 1.9249, Average Validation Loss: 1.9744\n",
      "Epoch 13: Average Training Loss: 1.9566, Average Validation Loss: 1.8285\n",
      "Epoch 14: Average Training Loss: 1.8855, Average Validation Loss: 1.8009\n",
      "Epoch 15: Average Training Loss: 1.8751, Average Validation Loss: 1.8891\n",
      "\n",
      "Evaluating on test set...\n",
      "Final Test Results:\n",
      "Average Test Loss: 2.0859\n",
      "\n",
      "[12/16] Testing parameters: {'d_model': 128, 'nhead': 8, 'num_layers': 16, 'learning_rate': 0.05, 'weight_decay': 0.01, 'batch_size': 8, 'max_length': 16}\n",
      "Using decoupled weight decay\n",
      "Epoch 1: Average Training Loss: 3.4629, Average Validation Loss: 3.4435\n",
      "Epoch 2: Average Training Loss: 3.4085, Average Validation Loss: 3.2739\n",
      "Epoch 3: Average Training Loss: 3.1719, Average Validation Loss: 2.8519\n",
      "Epoch 4: Average Training Loss: 2.7960, Average Validation Loss: 2.4659\n",
      "Epoch 5: Average Training Loss: 2.4845, Average Validation Loss: 2.2994\n",
      "Epoch 6: Average Training Loss: 2.2861, Average Validation Loss: 2.0780\n",
      "Epoch 7: Average Training Loss: 2.2110, Average Validation Loss: 2.0883\n",
      "Epoch 8: Average Training Loss: 2.1706, Average Validation Loss: 2.0967\n",
      "Epoch 9: Average Training Loss: 2.0888, Average Validation Loss: 2.0382\n",
      "Epoch 10: Average Training Loss: 2.0476, Average Validation Loss: 1.9909\n",
      "Epoch 11: Average Training Loss: 2.0178, Average Validation Loss: 1.9666\n",
      "Epoch 12: Average Training Loss: 2.0161, Average Validation Loss: 2.0091\n",
      "Epoch 13: Average Training Loss: 2.0132, Average Validation Loss: 1.8730\n",
      "Epoch 14: Average Training Loss: 1.9761, Average Validation Loss: 1.9488\n",
      "Epoch 15: Average Training Loss: 1.9812, Average Validation Loss: 1.9149\n",
      "\n",
      "Evaluating on test set...\n",
      "Final Test Results:\n",
      "Average Test Loss: 2.0303\n",
      "\n",
      "[13/16] Testing parameters: {'d_model': 128, 'nhead': 16, 'num_layers': 8, 'learning_rate': 0.1, 'weight_decay': 0.01, 'batch_size': 8, 'max_length': 16}\n",
      "Using decoupled weight decay\n",
      "Epoch 1: Average Training Loss: 3.4924, Average Validation Loss: 3.4612\n",
      "Epoch 2: Average Training Loss: 3.2642, Average Validation Loss: 2.8300\n",
      "Epoch 3: Average Training Loss: 2.4909, Average Validation Loss: 2.1126\n",
      "Epoch 4: Average Training Loss: 2.1654, Average Validation Loss: 1.9549\n",
      "Epoch 5: Average Training Loss: 2.1034, Average Validation Loss: 1.9736\n",
      "Epoch 6: Average Training Loss: 2.0466, Average Validation Loss: 1.9546\n",
      "Epoch 7: Average Training Loss: 2.0135, Average Validation Loss: 1.8837\n",
      "Epoch 8: Average Training Loss: 1.9407, Average Validation Loss: 1.8412\n",
      "Epoch 9: Average Training Loss: 1.9518, Average Validation Loss: 1.9568\n",
      "Epoch 10: Average Training Loss: 1.9225, Average Validation Loss: 1.8193\n",
      "Epoch 11: Average Training Loss: 1.9152, Average Validation Loss: 1.9129\n",
      "Epoch 12: Average Training Loss: 1.8898, Average Validation Loss: 1.7600\n",
      "Epoch 13: Average Training Loss: 1.8797, Average Validation Loss: 1.8729\n",
      "Epoch 14: Average Training Loss: 1.8251, Average Validation Loss: 1.8825\n",
      "Epoch 15: Average Training Loss: 1.8635, Average Validation Loss: 1.9290\n",
      "\n",
      "Early stopping triggered after epoch 15\n",
      "Best Validation Loss: 1.7600\n",
      "\n",
      "Evaluating on test set...\n",
      "Final Test Results:\n",
      "Average Test Loss: 1.9748\n",
      "\n",
      "[14/16] Testing parameters: {'d_model': 128, 'nhead': 16, 'num_layers': 8, 'learning_rate': 0.05, 'weight_decay': 0.01, 'batch_size': 8, 'max_length': 16}\n",
      "Using decoupled weight decay\n",
      "Epoch 1: Average Training Loss: 3.5158, Average Validation Loss: 3.4815\n",
      "Epoch 2: Average Training Loss: 3.4807, Average Validation Loss: 3.3561\n",
      "Epoch 3: Average Training Loss: 3.2707, Average Validation Loss: 2.8477\n",
      "Epoch 4: Average Training Loss: 2.7263, Average Validation Loss: 2.3570\n",
      "Epoch 5: Average Training Loss: 2.3597, Average Validation Loss: 2.1502\n",
      "Epoch 6: Average Training Loss: 2.1823, Average Validation Loss: 2.0703\n",
      "Epoch 7: Average Training Loss: 2.0576, Average Validation Loss: 1.9904\n",
      "Epoch 8: Average Training Loss: 2.0110, Average Validation Loss: 1.9818\n",
      "Epoch 9: Average Training Loss: 1.9854, Average Validation Loss: 1.9272\n",
      "Epoch 10: Average Training Loss: 1.9466, Average Validation Loss: 1.9452\n",
      "Epoch 11: Average Training Loss: 1.9476, Average Validation Loss: 1.9009\n",
      "Epoch 12: Average Training Loss: 1.9029, Average Validation Loss: 1.9103\n",
      "Epoch 13: Average Training Loss: 1.8642, Average Validation Loss: 1.8937\n",
      "Epoch 14: Average Training Loss: 1.8652, Average Validation Loss: 1.9370\n",
      "Epoch 15: Average Training Loss: 1.8562, Average Validation Loss: 1.8173\n",
      "\n",
      "Evaluating on test set...\n",
      "Final Test Results:\n",
      "Average Test Loss: 1.9820\n",
      "\n",
      "[15/16] Testing parameters: {'d_model': 128, 'nhead': 16, 'num_layers': 16, 'learning_rate': 0.1, 'weight_decay': 0.01, 'batch_size': 8, 'max_length': 16}\n",
      "Using decoupled weight decay\n",
      "Epoch 1: Average Training Loss: 3.3369, Average Validation Loss: 3.2941\n",
      "Epoch 2: Average Training Loss: 3.0963, Average Validation Loss: 2.7097\n",
      "Epoch 3: Average Training Loss: 2.5614, Average Validation Loss: 2.2088\n",
      "Epoch 4: Average Training Loss: 2.2606, Average Validation Loss: 2.1023\n",
      "Epoch 5: Average Training Loss: 2.1804, Average Validation Loss: 2.0773\n",
      "Epoch 6: Average Training Loss: 2.1376, Average Validation Loss: 2.0195\n",
      "Epoch 7: Average Training Loss: 2.0721, Average Validation Loss: 2.0262\n",
      "Epoch 8: Average Training Loss: 2.0523, Average Validation Loss: 2.1276\n",
      "Epoch 9: Average Training Loss: 2.0363, Average Validation Loss: 1.9131\n",
      "Epoch 10: Average Training Loss: 2.0318, Average Validation Loss: 1.9849\n",
      "Epoch 11: Average Training Loss: 1.9594, Average Validation Loss: 1.9287\n",
      "Epoch 12: Average Training Loss: 1.9516, Average Validation Loss: 1.8791\n",
      "Epoch 13: Average Training Loss: 1.9625, Average Validation Loss: 1.9468\n",
      "Epoch 14: Average Training Loss: 1.9569, Average Validation Loss: 1.8804\n",
      "Epoch 15: Average Training Loss: 1.9155, Average Validation Loss: 1.8752\n",
      "\n",
      "Evaluating on test set...\n",
      "Final Test Results:\n",
      "Average Test Loss: 2.0965\n",
      "\n",
      "[16/16] Testing parameters: {'d_model': 128, 'nhead': 16, 'num_layers': 16, 'learning_rate': 0.05, 'weight_decay': 0.01, 'batch_size': 8, 'max_length': 16}\n",
      "Using decoupled weight decay\n",
      "Epoch 1: Average Training Loss: 3.4554, Average Validation Loss: 3.4274\n",
      "Epoch 2: Average Training Loss: 3.4158, Average Validation Loss: 3.2974\n",
      "Epoch 3: Average Training Loss: 3.1943, Average Validation Loss: 2.9577\n",
      "Epoch 4: Average Training Loss: 2.9074, Average Validation Loss: 2.6912\n",
      "Epoch 5: Average Training Loss: 2.6566, Average Validation Loss: 2.3152\n",
      "Epoch 6: Average Training Loss: 2.3675, Average Validation Loss: 2.1663\n",
      "Epoch 7: Average Training Loss: 2.2497, Average Validation Loss: 2.1375\n",
      "Epoch 8: Average Training Loss: 2.1599, Average Validation Loss: 2.1111\n",
      "Epoch 9: Average Training Loss: 2.0895, Average Validation Loss: 2.0068\n",
      "Epoch 10: Average Training Loss: 2.0772, Average Validation Loss: 1.9093\n",
      "Epoch 11: Average Training Loss: 2.0148, Average Validation Loss: 1.9134\n",
      "Epoch 12: Average Training Loss: 1.9834, Average Validation Loss: 1.9313\n",
      "Epoch 13: Average Training Loss: 1.9729, Average Validation Loss: 1.8868\n",
      "Epoch 14: Average Training Loss: 1.9716, Average Validation Loss: 1.8813\n",
      "Epoch 15: Average Training Loss: 1.9600, Average Validation Loss: 1.9134\n",
      "\n",
      "Evaluating on test set...\n",
      "Final Test Results:\n",
      "Average Test Loss: 2.0907\n",
      "\n",
      "Grid search completed!\n",
      "Best parameters: {'d_model': 128, 'nhead': 16, 'num_layers': 8, 'learning_rate': 0.1, 'weight_decay': 0.01, 'batch_size': 8, 'max_length': 16}\n",
      "Best test loss: 1.9747720956802368\n"
     ]
    }
   ],
   "source": [
    "result = grid_search(\n",
    "    trn=trn,\n",
    "    val=val,\n",
    "    tst=tst,\n",
    "    param_grid=param_grid,\n",
    "    epochs=15,\n",
    "    patience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the best hyperparameters are already determined above, those parameters are used to train a final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using decoupled weight decay\n",
      "Epoch 1: Average Training Loss: 3.4464, Average Validation Loss: 3.3782\n",
      "Epoch 2: Average Training Loss: 3.2012, Average Validation Loss: 2.7766\n",
      "Epoch 3: Average Training Loss: 2.5506, Average Validation Loss: 2.2288\n",
      "Epoch 4: Average Training Loss: 2.1918, Average Validation Loss: 2.0165\n",
      "Epoch 5: Average Training Loss: 2.0739, Average Validation Loss: 1.9730\n",
      "Epoch 6: Average Training Loss: 2.0533, Average Validation Loss: 2.0382\n",
      "Epoch 7: Average Training Loss: 1.9835, Average Validation Loss: 1.9551\n",
      "Epoch 8: Average Training Loss: 1.9389, Average Validation Loss: 1.9197\n",
      "Epoch 9: Average Training Loss: 1.9323, Average Validation Loss: 1.9295\n",
      "Epoch 10: Average Training Loss: 1.9066, Average Validation Loss: 1.8553\n",
      "Epoch 11: Average Training Loss: 1.8833, Average Validation Loss: 1.8317\n",
      "Epoch 12: Average Training Loss: 1.8912, Average Validation Loss: 1.8517\n",
      "Epoch 13: Average Training Loss: 1.8744, Average Validation Loss: 1.8790\n",
      "Epoch 14: Average Training Loss: 1.8470, Average Validation Loss: 1.8768\n",
      "Epoch 15: Average Training Loss: 1.8269, Average Validation Loss: 1.8879\n",
      "Epoch 16: Average Training Loss: 1.8471, Average Validation Loss: 1.8293\n",
      "Epoch 17: Average Training Loss: 1.8215, Average Validation Loss: 1.7970\n",
      "Epoch 18: Average Training Loss: 1.8252, Average Validation Loss: 1.8255\n",
      "Epoch 19: Average Training Loss: 1.7924, Average Validation Loss: 1.8175\n",
      "Epoch 20: Average Training Loss: 1.7990, Average Validation Loss: 1.8264\n",
      "Epoch 21: Average Training Loss: 1.7863, Average Validation Loss: 1.8354\n",
      "Epoch 22: Average Training Loss: 1.7670, Average Validation Loss: 1.9150\n",
      "Epoch 23: Average Training Loss: 1.7663, Average Validation Loss: 1.7661\n",
      "Epoch 24: Average Training Loss: 1.7938, Average Validation Loss: 1.8833\n",
      "Epoch 25: Average Training Loss: 1.8015, Average Validation Loss: 1.7693\n",
      "Epoch 26: Average Training Loss: 1.7780, Average Validation Loss: 1.8594\n",
      "Epoch 27: Average Training Loss: 1.7947, Average Validation Loss: 1.7940\n",
      "Epoch 28: Average Training Loss: 1.7908, Average Validation Loss: 1.8107\n",
      "Epoch 29: Average Training Loss: 1.7860, Average Validation Loss: 1.7810\n",
      "Epoch 30: Average Training Loss: 1.7831, Average Validation Loss: 1.7442\n",
      "Epoch 31: Average Training Loss: 1.7599, Average Validation Loss: 1.8519\n",
      "Epoch 32: Average Training Loss: 1.7519, Average Validation Loss: 1.7115\n",
      "Epoch 33: Average Training Loss: 1.7339, Average Validation Loss: 1.8459\n",
      "Epoch 34: Average Training Loss: 1.7240, Average Validation Loss: 1.7997\n",
      "Epoch 35: Average Training Loss: 1.7298, Average Validation Loss: 1.7907\n",
      "Epoch 36: Average Training Loss: 1.7001, Average Validation Loss: 1.7343\n",
      "Epoch 37: Average Training Loss: 1.7000, Average Validation Loss: 1.8452\n",
      "Epoch 38: Average Training Loss: 1.7382, Average Validation Loss: 1.8727\n",
      "Epoch 39: Average Training Loss: 1.7220, Average Validation Loss: 1.7399\n",
      "Epoch 40: Average Training Loss: 1.7035, Average Validation Loss: 1.7667\n",
      "Epoch 41: Average Training Loss: 1.7067, Average Validation Loss: 1.8048\n",
      "Epoch 42: Average Training Loss: 1.6961, Average Validation Loss: 1.8222\n",
      "\n",
      "Early stopping triggered after epoch 42\n",
      "Best Validation Loss: 1.7115\n",
      "\n",
      "Evaluating on test set...\n",
      "Final Test Results:\n",
      "Average Test Loss: 2.0633\n"
     ]
    }
   ],
   "source": [
    "train_result = train(trn, val, tst, params=result[\"parameters\"], epochs=100, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.063342809677124"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_result['final_test_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model has been trained, testing is performed to manually check the performance of the model. This is done by randomly sampling the dataset for roundness values and manually comparing the roundness value to the generated pseudoword, as well as checking how much the generated pseudoword corresponds to the label\n",
    "\n",
    "After that, a list of roundness values are fed to the model across a range from 0 to 1. This will allow for manual checking of whether the generated pseudowords correspond to the roundness value input across the entire spectrum of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roundness Value : 0.4565217391304347\n",
      "Original Word   : guegui\n",
      "Predicted word  : foto\n",
      "\n",
      "Roundness Value : 0.3695652173913043\n",
      "Original Word   : sise\n",
      "Predicted word  : tutiu\n",
      "\n",
      "Roundness Value : 0.9130434782608696\n",
      "Original Word   : nonou\n",
      "Predicted word  : louo\n",
      "\n",
      "Roundness Value : 0.8695652173913043\n",
      "Original Word   : minlin\n",
      "Predicted word  : louo\n",
      "\n",
      "Roundness Value : 0.0\n",
      "Original Word   : zize\n",
      "Predicted word  : teke\n",
      "\n",
      "Roundness Value : 0.8695652173913043\n",
      "Original Word   : ama\n",
      "Predicted word  : louo\n",
      "\n",
      "Roundness Value : 0.3695652173913043\n",
      "Original Word   : kantan\n",
      "Predicted word  : tutiu\n",
      "\n",
      "Roundness Value : 0.9130434782608696\n",
      "Original Word   : umu\n",
      "Predicted word  : louo\n",
      "\n",
      "Roundness Value : 0.9130434782608696\n",
      "Original Word   : ulu\n",
      "Predicted word  : louo\n",
      "\n",
      "Roundness Value : 0.1847826086956521\n",
      "Original Word   : chechi\n",
      "Predicted word  : tute\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_sample = data.sample(n=10, random_state=42)\n",
    "for _, row in random_sample.iterrows():\n",
    "    print(f\"Roundness Value : {row['Roundness']}\")\n",
    "    print(f\"Original Word   : {row['Pseudoword']}\")\n",
    "    print(f\"Predicted word  : {inference(train_result['model'], row['Roundness'], train_result['tokenizer'])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roundness Value: 0.0\n",
      "Predicted word: teke\n",
      "\n",
      "Roundness Value: 0.1\n",
      "Predicted word: teke\n",
      "\n",
      "Roundness Value: 0.2\n",
      "Predicted word: tute\n",
      "\n",
      "Roundness Value: 0.3\n",
      "Predicted word: tute\n",
      "\n",
      "Roundness Value: 0.4\n",
      "Predicted word: tuto\n",
      "\n",
      "Roundness Value: 0.5\n",
      "Predicted word: fofo\n",
      "\n",
      "Roundness Value: 0.6\n",
      "Predicted word: fofou\n",
      "\n",
      "Roundness Value: 0.7\n",
      "Predicted word: jolou\n",
      "\n",
      "Roundness Value: 0.8\n",
      "Predicted word: louou\n",
      "\n",
      "Roundness Value: 0.9\n",
      "Predicted word: louo\n",
      "\n",
      "Roundness Value: 1.0\n",
      "Predicted word: louo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "roundness_list = []\n",
    "for i in range(11):\n",
    "    roundness_list.append(i/10)\n",
    "\n",
    "for roundness in roundness_list:\n",
    "    print(f\"Roundness Value: {roundness}\")\n",
    "    print(f\"Predicted word: {inference(train_result['model'], roundness, train_result['tokenizer'])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(train_result['model'], path=f\"outputs/pseudoword_generator_v0{os.getenv(\"GEN\")}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"outputs/params_for_model_v0{os.getenv(\"GEN\")}.json\", \"w\") as f:\n",
    "    json.dump(result[\"parameters\"], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(filename=f\"pseudoword_generator_v0{os.getenv(\"GEN\")}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pseudoword",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
