{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an decoder model which will take in a roundness value and output a pseudoword that corresponds to the roundness value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pseudoword_generator import *\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "pd.set_option('display.max_columns', None)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pseudoword</th>\n",
       "      <th>Roundness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>irepeo</td>\n",
       "      <td>0.481854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bea</td>\n",
       "      <td>0.562286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kiko</td>\n",
       "      <td>0.371240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tsupihamumo</td>\n",
       "      <td>0.235215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>koke</td>\n",
       "      <td>0.212680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>tademunoo</td>\n",
       "      <td>0.782394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>tsujidenubo</td>\n",
       "      <td>0.339519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>musa</td>\n",
       "      <td>0.517814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>sateihemu</td>\n",
       "      <td>0.492699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>pekene</td>\n",
       "      <td>0.458166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pseudoword  Roundness\n",
       "0          irepeo   0.481854\n",
       "1             bea   0.562286\n",
       "2            kiko   0.371240\n",
       "3     tsupihamumo   0.235215\n",
       "4            koke   0.212680\n",
       "...           ...        ...\n",
       "9995    tademunoo   0.782394\n",
       "9996  tsujidenubo   0.339519\n",
       "9997         musa   0.517814\n",
       "9998    sateihemu   0.492699\n",
       "9999       pekene   0.458166\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataset\n",
    "data = pd.read_csv(f\"datasets/japanese_pseudowords.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Roundness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.169169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.172221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.372915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.494395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.613485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.891836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Roundness\n",
       "count  10000.000000\n",
       "mean       0.500261\n",
       "std        0.169169\n",
       "min        0.172221\n",
       "25%        0.372915\n",
       "50%        0.494395\n",
       "75%        0.613485\n",
       "max        0.891836"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, val and test sets\n",
    "\n",
    "trn = data.sample(frac=0.8, random_state=state)\n",
    "val = data.drop(trn.index).sample(frac=0.5, random_state=state)\n",
    "tst = data.drop(trn.index).drop(val.index)\n",
    "trn.reset_index(inplace=True, drop=True)\n",
    "val.reset_index(inplace=True, drop=True)\n",
    "tst.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 8000 samples, Validation set: 1000 samples, Test set: 1000 samples\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train set: {len(trn)} samples, Validation set: {len(val)} samples, Test set: {len(tst)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model = RoundnessToTextModel(\n",
    "    t5_model_name=\"sonoisa/t5-base-japanese\",\n",
    "    freeze_t5=False,\n",
    "    hidden_dim=1024,\n",
    "    output_dim=768,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/100, Train Loss: 18.8334, Validation Loss: 15.1789, Best Val Loss: 15.1789\n",
      "Epoch   2/100, Train Loss: 14.8226, Validation Loss: 13.9091, Best Val Loss: 13.9091\n",
      "Epoch   3/100, Train Loss: 13.2584, Validation Loss: 12.0886, Best Val Loss: 12.0886\n",
      "Epoch   4/100, Train Loss: 11.4614, Validation Loss: 10.2595, Best Val Loss: 10.2595\n",
      "Epoch   5/100, Train Loss: 9.0821, Validation Loss: 6.7239, Best Val Loss: 6.7239\n",
      "Epoch   6/100, Train Loss: 5.3358, Validation Loss: 3.0156, Best Val Loss: 3.0156\n",
      "Epoch   7/100, Train Loss: 3.4260, Validation Loss: 2.8728, Best Val Loss: 2.8728\n",
      "Epoch   8/100, Train Loss: 3.2642, Validation Loss: 2.7708, Best Val Loss: 2.7708\n",
      "Epoch   9/100, Train Loss: 3.0557, Validation Loss: 2.7294, Best Val Loss: 2.7294\n",
      "Epoch  10/100, Train Loss: 2.9575, Validation Loss: 2.7125, Best Val Loss: 2.7125\n",
      "Epoch  11/100, Train Loss: 2.8975, Validation Loss: 2.6951, Best Val Loss: 2.6951\n",
      "Epoch  12/100, Train Loss: 2.8484, Validation Loss: 2.6778, Best Val Loss: 2.6778\n",
      "Epoch  13/100, Train Loss: 2.8104, Validation Loss: 2.6686, Best Val Loss: 2.6686\n",
      "Epoch  14/100, Train Loss: 2.7771, Validation Loss: 2.6627, Best Val Loss: 2.6627\n",
      "Epoch  15/100, Train Loss: 2.7569, Validation Loss: 2.6552, Best Val Loss: 2.6552\n",
      "Epoch  16/100, Train Loss: 2.7407, Validation Loss: 2.6508, Best Val Loss: 2.6508\n",
      "Epoch  17/100, Train Loss: 2.7350, Validation Loss: 2.6472, Best Val Loss: 2.6472\n",
      "Epoch  18/100, Train Loss: 2.7218, Validation Loss: 2.6439, Best Val Loss: 2.6439\n",
      "Epoch  19/100, Train Loss: 2.7178, Validation Loss: 2.6406, Best Val Loss: 2.6406\n",
      "Epoch  20/100, Train Loss: 2.7131, Validation Loss: 2.6389, Best Val Loss: 2.6389\n",
      "Epoch  21/100, Train Loss: 2.7034, Validation Loss: 2.6363, Best Val Loss: 2.6363\n",
      "Epoch  22/100, Train Loss: 2.7059, Validation Loss: 2.6349, Best Val Loss: 2.6349\n",
      "Epoch  23/100, Train Loss: 2.7004, Validation Loss: 2.6341, Best Val Loss: 2.6341\n",
      "Epoch  24/100, Train Loss: 2.6980, Validation Loss: 2.6332, Best Val Loss: 2.6332\n",
      "Epoch  25/100, Train Loss: 2.6969, Validation Loss: 2.6325, Best Val Loss: 2.6325\n",
      "Epoch  26/100, Train Loss: 2.6952, Validation Loss: 2.6325, Best Val Loss: 2.6325\n",
      "Epoch  27/100, Train Loss: 2.6886, Validation Loss: 2.6311, Best Val Loss: 2.6311\n",
      "Epoch  28/100, Train Loss: 2.6925, Validation Loss: 2.6302, Best Val Loss: 2.6302\n",
      "Epoch  29/100, Train Loss: 2.6890, Validation Loss: 2.6299, Best Val Loss: 2.6299\n",
      "Epoch  30/100, Train Loss: 2.6892, Validation Loss: 2.6295, Best Val Loss: 2.6295\n",
      "Epoch  31/100, Train Loss: 2.6923, Validation Loss: 2.6295, Best Val Loss: 2.6295\n",
      "Epoch  32/100, Train Loss: 2.6883, Validation Loss: 2.6292, Best Val Loss: 2.6292\n",
      "Epoch  33/100, Train Loss: 2.6861, Validation Loss: 2.6290, Best Val Loss: 2.6290\n",
      "Epoch  34/100, Train Loss: 2.6905, Validation Loss: 2.6288, Best Val Loss: 2.6288\n",
      "Epoch  35/100, Train Loss: 2.6852, Validation Loss: 2.6285, Best Val Loss: 2.6285\n",
      "Epoch  36/100, Train Loss: 2.6877, Validation Loss: 2.6282, Best Val Loss: 2.6282\n",
      "Epoch  37/100, Train Loss: 2.6858, Validation Loss: 2.6280, Best Val Loss: 2.6280\n",
      "Epoch  38/100, Train Loss: 2.6872, Validation Loss: 2.6279, Best Val Loss: 2.6279\n",
      "Epoch  39/100, Train Loss: 2.6881, Validation Loss: 2.6278, Best Val Loss: 2.6278\n",
      "Epoch  40/100, Train Loss: 2.6898, Validation Loss: 2.6277, Best Val Loss: 2.6277\n",
      "Epoch  41/100, Train Loss: 2.6860, Validation Loss: 2.6277, Best Val Loss: 2.6277\n",
      "Epoch  42/100, Train Loss: 2.6863, Validation Loss: 2.6277, Best Val Loss: 2.6277\n",
      "Epoch  43/100, Train Loss: 2.6879, Validation Loss: 2.6277, Best Val Loss: 2.6277\n",
      "Epoch  44/100, Train Loss: 2.6865, Validation Loss: 2.6276, Best Val Loss: 2.6276\n",
      "Epoch  45/100, Train Loss: 2.6927, Validation Loss: 2.6276, Best Val Loss: 2.6276\n",
      "Epoch  46/100, Train Loss: 2.6859, Validation Loss: 2.6276, Best Val Loss: 2.6276\n",
      "Epoch  47/100, Train Loss: 2.6881, Validation Loss: 2.6276, Best Val Loss: 2.6276\n",
      "Epoch  48/100, Train Loss: 2.6829, Validation Loss: 2.6276, Best Val Loss: 2.6276\n",
      "Epoch  49/100, Train Loss: 2.6881, Validation Loss: 2.6276, Best Val Loss: 2.6276\n",
      "Epoch  50/100, Train Loss: 2.6873, Validation Loss: 2.6276, Best Val Loss: 2.6276\n",
      "Epoch  51/100, Train Loss: 2.6843, Validation Loss: 2.6276, Best Val Loss: 2.6276\n",
      "Epoch  52/100, Train Loss: 2.6869, Validation Loss: 2.6276, Best Val Loss: 2.6276\n",
      "Epoch  53/100, Train Loss: 2.6848, Validation Loss: 2.6276, Best Val Loss: 2.6276\n",
      "Epoch  54/100, Train Loss: 2.6902, Validation Loss: 2.6276, Best Val Loss: 2.6276\n",
      "Epoch  55/100, Train Loss: 2.6862, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  56/100, Train Loss: 2.6835, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  57/100, Train Loss: 2.6832, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  58/100, Train Loss: 2.6890, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  59/100, Train Loss: 2.6848, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  60/100, Train Loss: 2.6853, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  61/100, Train Loss: 2.6839, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  62/100, Train Loss: 2.6872, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  63/100, Train Loss: 2.6875, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  64/100, Train Loss: 2.6830, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  65/100, Train Loss: 2.6827, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  66/100, Train Loss: 2.6842, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  67/100, Train Loss: 2.6815, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  68/100, Train Loss: 2.6855, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  69/100, Train Loss: 2.6838, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  70/100, Train Loss: 2.6866, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  71/100, Train Loss: 2.6846, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  72/100, Train Loss: 2.6860, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  73/100, Train Loss: 2.6862, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  74/100, Train Loss: 2.6874, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  75/100, Train Loss: 2.6842, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  76/100, Train Loss: 2.6871, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  77/100, Train Loss: 2.6826, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  78/100, Train Loss: 2.6852, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  79/100, Train Loss: 2.6861, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  80/100, Train Loss: 2.6861, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  81/100, Train Loss: 2.6917, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  82/100, Train Loss: 2.6885, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  83/100, Train Loss: 2.6889, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  84/100, Train Loss: 2.6862, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Epoch  85/100, Train Loss: 2.6861, Validation Loss: 2.6275, Best Val Loss: 2.6275\n",
      "Early stopping triggered after 85 epochs\n",
      "Test Loss: 2.5496\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    trn_roundness=trn[\"Roundness\"],\n",
    "    val_roundness=val[\"Roundness\"],\n",
    "    tst_roundness=tst[\"Roundness\"],\n",
    "    trn_texts=trn[\"Pseudoword\"],\n",
    "    val_texts=val[\"Pseudoword\"],\n",
    "    tst_texts=tst[\"Pseudoword\"],\n",
    "    batch_size=min(len(val), 1000),\n",
    "    epochs=100,\n",
    "    patience=10,\n",
    "    scheduler=scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roundness Value : 0.2795007\n",
      "Original Word   : koshi\n",
      "Predicted word  : nnupa\n",
      "\n",
      "Roundness Value : 0.42155555\n",
      "Original Word   : mifufude\n",
      "Predicted word  : pubabafu\n",
      "\n",
      "Roundness Value : 0.7146631\n",
      "Original Word   : ize\n",
      "Predicted word  : neeshi\n",
      "\n",
      "Roundness Value : 0.6702508\n",
      "Original Word   : rapueroi\n",
      "Predicted word  : gurebo\n",
      "\n",
      "Roundness Value : 0.6265058\n",
      "Original Word   : oge\n",
      "Predicted word  : mipako\n",
      "\n",
      "Roundness Value : 0.6067172\n",
      "Original Word   : daruusuku\n",
      "Predicted word  : migatsu\n",
      "\n",
      "Roundness Value : 0.40251696\n",
      "Original Word   : mote\n",
      "Predicted word  : mojiro\n",
      "\n",
      "Roundness Value : 0.8509308\n",
      "Original Word   : yabodoo\n",
      "Predicted word  : kie\n",
      "\n",
      "Roundness Value : 0.64203817\n",
      "Original Word   : jiso\n",
      "Predicted word  : ozo\n",
      "\n",
      "Roundness Value : 0.75613195\n",
      "Original Word   : oeido\n",
      "Predicted word  : tsuku\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_sample = tst.sample(n=10, random_state=42)\n",
    "for _, row in random_sample.iterrows():\n",
    "    print(f\"Roundness Value : {row['Roundness']}\")\n",
    "    print(f\"Original Word   : {row['Pseudoword']}\")\n",
    "    print(f\"Predicted word  : {inference(model, row[\"Roundness\"])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roundness Value: 0.0\n",
      "Predicted word: keguchifu\n",
      "\n",
      "Roundness Value: 0.1\n",
      "Predicted word: wamu\n",
      "\n",
      "Roundness Value: 0.2\n",
      "Predicted word: kuhe\n",
      "\n",
      "Roundness Value: 0.3\n",
      "Predicted word: remitsumoba\n",
      "\n",
      "Roundness Value: 0.4\n",
      "Predicted word: kajihe\n",
      "\n",
      "Roundness Value: 0.5\n",
      "Predicted word: gebayo\n",
      "\n",
      "Roundness Value: 0.6\n",
      "Predicted word: noeya\n",
      "\n",
      "Roundness Value: 0.7\n",
      "Predicted word: nadesu\n",
      "\n",
      "Roundness Value: 0.8\n",
      "Predicted word: tahodo\n",
      "\n",
      "Roundness Value: 0.9\n",
      "Predicted word: genodo\n",
      "\n",
      "Roundness Value: 1.0\n",
      "Predicted word: nitano\n",
      "\n"
     ]
    }
   ],
   "source": [
    "roundness_list = []\n",
    "for i in range(11):\n",
    "    roundness_list.append(i/10)\n",
    "\n",
    "for roundness in roundness_list:\n",
    "    print(f\"Roundness Value: {roundness}\")\n",
    "    print(f\"Predicted word: {inference(model, roundness)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to outputs/pseudoword_generator_v03.2.pth\n"
     ]
    }
   ],
   "source": [
    "save_model(model, filename=f\"pseudoword_generator_v0{os.getenv(\"GEN\")}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(filename=f\"pseudoword_generator_v0{os.getenv(\"GEN\")}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pseudoword",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
